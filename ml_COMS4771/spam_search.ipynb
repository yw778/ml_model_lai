{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding = 'latin' # utf8 cannot decode\n",
    "train_path = './email_classification_data/train_data'\n",
    "X_train, y_train = [], []\n",
    "ham_path = train_path + '/ham'\n",
    "spam_path = train_path + '/spam'\n",
    "for file in os.listdir(ham_path):\n",
    "    s = open(ham_path + '/' + file, encoding=encoding).read()\n",
    "    X_train.append(s)\n",
    "    y_train.append(0)\n",
    "for file in os.listdir(spam_path):\n",
    "    s = open(spam_path + '/' + file, encoding=encoding).read()\n",
    "    X_train.append(s)\n",
    "    y_train.append(1)\n",
    "\n",
    "test_path = './email_classification_data/test_data'\n",
    "X_test = []\n",
    "idx_test = []\n",
    "for file in os.listdir(test_path):\n",
    "    s = open(test_path + '/' + file, encoding=encoding).read()\n",
    "    X_test.append(s)\n",
    "    idx_test.append(file[11:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4372"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trn, X_val, y_trn, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3497"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3497, 40780)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_trn_counts = count_vect.fit_transform(X_trn)\n",
    "X_trn_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      1.00      0.92       641\n",
      "          1       1.00      0.53      0.70       234\n",
      "\n",
      "avg / total       0.89      0.88      0.86       875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_trn_tfidf = tfidf_transformer.fit_transform(X_trn_counts)\n",
    "X_val_counts = count_vect.transform(X_val)\n",
    "X_val_tfidf = tfidf_transformer.transform(X_val_counts)\n",
    "clf = MultinomialNB().fit(X_trn_tfidf, y_trn)\n",
    "pred_y_val = clf.predict(X_val_tfidf)\n",
    "print(classification_report(y_true = y_val, y_pred = pred_y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### easy pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(txt_clf):\n",
    "    scores = cross_val_score(estimator = txt_clf, X = X_train, y = y_train, cv=5)\n",
    "    print(scores)\n",
    "    print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97371429  0.98171429  0.9805492   0.98283753  0.96567506]\n",
      "0.976898071265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "txt_clf =  Pipeline([('vect', CountVectorizer(max_df = 0.5, max_features = None, ngram_range = (1,2))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf = True, norm = 'l2')),\n",
    "                     ('clf', SGDClassifier(penalty = 'l2',n_iter = 10, alpha = 1e-05 ))])\n",
    "validate(txt_clf)\n",
    "# produce(txt_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (1e-05, 1e-06),\n",
      " 'clf__n_iter': (10, 50, 80),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'vect__max_df': (0.5, 0.75),\n",
      " 'vect__max_features': (None, 5000, 10000, 50000),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   48.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed: 16.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1011.785s\n",
      "\n",
      "Best score: 0.964\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-05\n",
      "\tclf__n_iter: 10\n",
      "\tclf__penalty: 'l2'\n",
      "\tvect__max_df: 0.75\n",
      "\tvect__max_features: 50000\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Best score: 0.980\n",
    "# Best parameters set:\n",
    "# \tclf__alpha: 1e-05\n",
    "# \tclf__n_iter: 10\n",
    "# \tclf__penalty: 'l2'\n",
    "# \ttfidf__norm: 'l2'\n",
    "# \ttfidf__use_idf: True\n",
    "# \tvect__max_df: 0.5\n",
    "# \tvect__max_features: None\n",
    "# \tvect__ngram_range: (1, 2)\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "### the following paramter of clf must define according to different model\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "gridSearchParameter(txt_clf, parameters)\n",
    "# randomSearchParameter(txt_clf, parameters, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96        0.968       0.97254005  0.96910755  0.95423341]\n",
      "0.964776201373\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "txt_clf =  Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf', LogisticRegression())])\n",
    "validate(txt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96914286  0.98057143  0.97368421  0.96567506  0.96453089]\n",
      "0.970720889179\n"
     ]
    }
   ],
   "source": [
    "txt_clf =  Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB(alpha=0.1))])\n",
    "validate(txt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97028571  0.98171429  0.96796339  0.96567506  0.96453089]\n",
      "0.970033867277\n"
     ]
    }
   ],
   "source": [
    "txt_clf =  Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB(alpha=1.0))])\n",
    "validate(txt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.968       0.98057143  0.97597254  0.97025172  0.96224256]\n",
      "0.971407649559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "txt_clf =  Pipeline([\n",
    "#                      ('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "                    ('TFIDF_VEC', TfidfVectorizer()),\n",
    "                     ('clf', MultinomialNB(alpha=0.01))])\n",
    "validate(txt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96685714  0.97371429  0.96453089  0.96224256  0.96567506]\n",
      "0.966603988231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "txt_clf =  Pipeline([\n",
    "    ('TFIDF_VEC', TfidfVectorizer(norm='l2', sublinear_tf=True)),\n",
    "    ('clf', MultinomialNB(alpha=0.1))\n",
    "])\n",
    "validate(txt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94285714  0.936       0.92791762  0.91647597  0.91075515]\n",
      "0.926801176855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "txt_clf =  Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier())])\n",
    "validate(txt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93942857  0.92342857  0.95308924  0.93363844  0.90846682]\n",
      "0.931610330173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "txt_clf =  Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf', RandomForestClassifier())])\n",
    "validate(txt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_clf =  Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', AdaBoostClassifier())])\n",
    "validate(txt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_clf =  Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf', AdaBoostClassifier())])\n",
    "validate(txt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_clf =  Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier())])\n",
    "validate(txt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Gridsearch and Randomsearch for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "def gridSearchParameter(pipline, parameters):\n",
    "    \n",
    "    grid_search = GridSearchCV(pipline, parameters, n_jobs=-1, verbose=1, cv=5)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "def randomSearchParameter(pipline, parameters, max_iter = 5):\n",
    "    \n",
    "    random_search = RandomizedSearchCV(pipline, parameters, n_jobs=-1, verbose=1, cv=5, n_iter = max_iter)\n",
    "\n",
    "    print(\"Performing random search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % random_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = random_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model ensemble\n",
    "失败了。。我没调好你可以删了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # lb = 0.97500\n",
    "txt_clf1 =  Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression())])\n",
    "\n",
    "# # lb = 0.98750\n",
    "txt_clf2 =  Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB(alpha=0.1))])\n",
    "\n",
    "# # lb = 0.97500 ********************** best leaderboard\n",
    "txt_clf3 =  Pipeline([\n",
    "    ('TFIDF_VEC', TfidfVectorizer(norm='l2', sublinear_tf=True)),\n",
    "    ('clf', MultinomialNB(alpha=0.1))\n",
    "])\n",
    "\n",
    "# # lb = 0.97916\n",
    "txt_clf4 =  Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [0, 1, 3, 4, 5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [1, 2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-e949d089bb0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "a[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-7108cad56264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdataset_blend_test_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mX_train_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0my_train_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mX_test_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "n_folds = 5\n",
    "skf = list(StratifiedKFold(y_train, n_folds))\n",
    "## model put here:\n",
    "clfs = [txt_clf1, txt_clf2, txt_clf3, txt_clf4]\n",
    "\n",
    "dataset_blend_train = np.zeros((len(X_train), len(clfs)))\n",
    "dataset_blend_test = np.zeros((len(X_test), len(clfs)))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    print (j, clf)\n",
    "    dataset_blend_test_j = np.zeros((len(X_test), len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        X_train_fold = X_train[train]\n",
    "        y_train_fold = y_train[train]\n",
    "        X_test_fold = X_train[test]\n",
    "        y_test_fold = y_train[test]\n",
    "        clf.fit(X_train_fold, y_train_fold)\n",
    "        y_submission = clf.predict_proba(X_test_fold)[:, 1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_test)[:, 1]\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "\n",
    "print\n",
    "print (\"Blending.\")\n",
    "clf = LogisticRegression()\n",
    "clf.fit(dataset_blend_train, y_train)\n",
    "# y_submission = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    "y_submission = clf.predict(dataset_blend_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_clf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce(txt_clf):\n",
    "    txt_clf.fit(X_train, y_train)\n",
    "    pred_y_test = txt_clf.predict(X_test)\n",
    "    answer = np.stack([idx_test, pred_y_test], axis=-1)\n",
    "    print(answer)\n",
    "    with open('output/' + datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\") + '.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(['email_id', 'labels'])\n",
    "        writer.writerows(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # lb = 0.97500\n",
    "# txt_clf =  Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('clf', LogisticRegression())])\n",
    "\n",
    "# # lb = 0.98750\n",
    "# txt_clf =  Pipeline([('vect', CountVectorizer()),\n",
    "# #                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('clf', MultinomialNB(alpha=0.1))])\n",
    "\n",
    "# # lb = 0.97500 ********************** best leaderboard\n",
    "# txt_clf =  Pipeline([\n",
    "#     ('TFIDF_VEC', TfidfVectorizer(norm='l2', sublinear_tf=True)),\n",
    "#     ('clf', MultinomialNB(alpha=0.1))\n",
    "# ])\n",
    "\n",
    "# # lb = 0.97916\n",
    "# txt_clf =  Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('clf', LogisticRegression())])\n",
    "\n",
    "\n",
    "\n",
    "produce(txt_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
